{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.style as style\n",
    "style.use('seaborn-whitegrid')\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.applications import VGG16\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing import image as image_utils\n",
    "from keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures = {'L_': 'L',\n",
    "           'fi': 'Fist',\n",
    "           'C_': 'C',\n",
    "           'ok': 'Okay',\n",
    "           'pe': 'Peace',\n",
    "           'pa': 'Palm'\n",
    "            }\n",
    "\n",
    "gestures_map = {'C': 0,\n",
    "               'Fist' : 1,\n",
    "               'L': 2,\n",
    "               'Okay': 3,\n",
    "               'Palm': 4,\n",
    "               'Peace': 5\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "image_path = []\n",
    "gesture = []\n",
    "image_rgb = []\n",
    "\n",
    "# directory_in_str = '/home/ubuntu/project_kojak/frames/silhouettes'\n",
    "directory = os.fsencode('/home/ubuntu/project_kojak/frames/drawings/')\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".jpg\"): \n",
    "        path = os.path.join('/home/ubuntu/project_kojak/frames/drawings/', filename)\n",
    "        gesture_name = gestures[filename[8:10]]\n",
    "        \n",
    "        gesture.append(gesture_name)\n",
    "        y_data.append(gestures_map[gesture_name])\n",
    "        image_path.append(path)\n",
    "\n",
    "        \n",
    "        img = Image.open(path).convert('L')\n",
    "        img = img.resize((224, 224))\n",
    "        arr = np.array(img)\n",
    "        X_data.append(arr)\n",
    "        \n",
    "        img2rgb = image_utils.load_img(path=path, target_size=(224, 224))\n",
    "        img2rgb = image_utils.img_to_array(img2rgb)\n",
    "        image_rgb.append(img2rgb)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#         y_values = np.full((count, 1), lookup[j]) \n",
    "#         y_data.append(y_values)\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "image_rgb = np.array(image_rgb, dtype = 'float32')\n",
    "# x_data = np.array(x_data, dtype=np.uint8)\n",
    "image_rgb = image_rgb.reshape((len(image_rgb), 224, 224, 3))\n",
    "image_rgb /= 255\n",
    "\n",
    "y_data = np.array(y_data)\n",
    "y_data = to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouettes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "image_path = []\n",
    "gesture = []\n",
    "image_rgb = []\n",
    "\n",
    "# directory_in_str = '/home/ubuntu/project_kojak/frames/silhouettes'\n",
    "directory = os.fsencode('/home/ubuntu/project_kojak/frames/silhouettes')\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".jpg\"): \n",
    "        path = os.path.join('/home/ubuntu/project_kojak/frames/silhouettes', filename)\n",
    "        gesture_name = gestures[filename[:2]]\n",
    "        \n",
    "        gesture.append(gesture_name)\n",
    "        y_data.append(gestures_map[gesture_name])\n",
    "        image_path.append(path)\n",
    "\n",
    "        \n",
    "        img = Image.open(path).convert('L')\n",
    "        img = img.resize((224, 224))\n",
    "        arr = np.array(img)\n",
    "        X_data.append(arr)\n",
    "        \n",
    "        img2rgb = image_utils.load_img(path=path, target_size=(224, 224))\n",
    "        img2rgb = image_utils.img_to_array(img2rgb)\n",
    "        image_rgb.append(img2rgb)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#         y_values = np.full((count, 1), lookup[j]) \n",
    "#         y_data.append(y_values)\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "image_rgb = np.array(image_rgb, dtype = 'float32')\n",
    "# x_data = np.array(x_data, dtype=np.uint8)\n",
    "image_rgb = image_rgb.reshape((len(image_rgb), 224, 224, 3))\n",
    "image_rgb /= 255\n",
    "\n",
    "y_data = np.array(y_data)\n",
    "y_data = to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(x_data, y_data):\n",
    "    x_data = np.array(x_data, dtype = 'float32')\n",
    "    # x_data = np.array(x_data, dtype=np.uint8)\n",
    "    x_data = x_data.reshape((len(x_data), 224, 224, 1))\n",
    "    x_data /= 255\n",
    "    \n",
    "    y_data = np.array(y_data)\n",
    "    y_data = y_data.reshape(len(x_data), 1)\n",
    "    y_data = to_categorical(y_data)\n",
    "    return x_data, y_data\n",
    "\n",
    "def process_data_rgb(x_data, y_data):\n",
    "    x_data = np.array(x_data, dtype = 'float32')\n",
    "    # x_data = np.array(x_data, dtype=np.uint8)\n",
    "    x_data = x_data.reshape((len(x_data), 224, 224, 3))\n",
    "    x_data /= 255\n",
    "    \n",
    "    y_data = np.array(y_data)\n",
    "    y_data = to_categorical(y_data)\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the dictionaries to a dataframe to be saved for future use\n",
    "d = {'image_path':image_path, 'gesture':gesture, 'image_rgb': image_rgb, 'image_bw_x': X_data, 'image_bw_y': y_data}\n",
    "df = pd.DataFrame(d)\n",
    "# df['gesture_num'] = df['gesture'].apply(lambda x: x[1:2])\n",
    "# df['gesture_name'] = df['gesture'].apply(lambda x: x[3:])\n",
    "\n",
    "# df.to_csv('silhouette_df.csv')\n",
    "df = pd.read_csv('silhouette_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data, y_data = process_data(X_data, y_data)\n",
    "# image_rgb, y_data = process_data_rgb(image_rgb, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rgb, X_test_rgb, y_train_rgb, y_test_rgb = train_test_split(image_rgb, y_data, test_size = 0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rgb, y_train_rgb = process_data_rgb(X_train_rgb, y_train_rgb)\n",
    "X_test_rgb, y_test_rgb = process_data_rgb(X_test_rgb, y_test_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/preprocessing/image.py:440: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  image.ImageDataGenerator.__init__).args:\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/utils/data_utils.py:651: DeprecationWarning: `wait_time` is not used anymore.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "19/18 [==============================] - 13s 697ms/step - loss: 1.7208 - acc: 0.2992 - val_loss: 1.1924 - val_acc: 0.6128\n",
      "Epoch 2/8\n",
      "19/18 [==============================] - 12s 652ms/step - loss: 1.0182 - acc: 0.6347 - val_loss: 0.5280 - val_acc: 0.8485\n",
      "Epoch 3/8\n",
      "19/18 [==============================] - 12s 657ms/step - loss: 0.6081 - acc: 0.7697 - val_loss: 0.4251 - val_acc: 0.8451\n",
      "Epoch 4/8\n",
      "19/18 [==============================] - 13s 658ms/step - loss: 0.3924 - acc: 0.8663 - val_loss: 0.2187 - val_acc: 0.9226\n",
      "Epoch 5/8\n",
      "19/18 [==============================] - 13s 659ms/step - loss: 0.3418 - acc: 0.8867 - val_loss: 0.1596 - val_acc: 0.9529\n",
      "Epoch 6/8\n",
      "19/18 [==============================] - 12s 655ms/step - loss: 0.2408 - acc: 0.9238 - val_loss: 0.2789 - val_acc: 0.9091\n",
      "Epoch 7/8\n",
      "19/18 [==============================] - 13s 660ms/step - loss: 0.2518 - acc: 0.9147 - val_loss: 0.3006 - val_acc: 0.8990\n",
      "Epoch 8/8\n",
      "19/18 [==============================] - 13s 658ms/step - loss: 0.2469 - acc: 0.9258 - val_loss: 0.1249 - val_acc: 0.9562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ea43b20f0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load VGG16\n",
    "#Get back the convolutional part of a VGG network trained on ImageNet\n",
    "import keras\n",
    "from keras import models, layers, optimizers\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization\n",
    "from keras.models import Model\n",
    "imageSize=224\n",
    "model1 = VGG16(weights='imagenet', include_top=False, input_shape=(imageSize, imageSize, 3))\n",
    "optimizer1 = optimizers.Adam()\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "featurewise_center=True,\n",
    "featurewise_std_normalization=True,\n",
    "rotation_range=45.,\n",
    "width_shift_range=0.2,\n",
    "height_shift_range=0.2,\n",
    "horizontal_flip=True)\n",
    "\n",
    "base_model = model1 # Topless\n",
    "# Add top layer\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu', name='fc1')(x)\n",
    "x = Dense(128, activation='relu', name='fc2')(x)\n",
    "x = Dense(128, activation='relu', name='fc3')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "#### Playing with other architectures\n",
    "#     x = Flatten() (x)\n",
    "#     x = Dense(64) (x)\n",
    "#     x = Activation('relu') (x)\n",
    "#     x = Dropout(0.5) (x)\n",
    "#     x = Dense(32) (x)\n",
    "#     x = Activation('relu') (x)\n",
    "#     x = Dropout(0.5) (x)\n",
    "####\n",
    "predictions = Dense(6, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Train top layer\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "      optimizer=optimizers.Adam(), \n",
    "      metrics=['accuracy'])\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "\n",
    "datagen.fit(X_train_rgb)\n",
    "\n",
    "# Fit model\n",
    "model.fit_generator(datagen.flow(X_train_rgb, y_train_rgb, batch_size=64),\n",
    "                steps_per_epoch=len(X_train_rgb) / 64, epochs=8, validation_data=(X_test_rgb, y_test_rgb))\n",
    "#class_weight=classweight,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/project_kojak\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('silhouette_VGG.h5')\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model('/home/ubuntu/project_kojak/silhouette_VGG.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_rgb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.5975669e-03, 6.8540269e-01, 8.0532441e-03, 4.6215006e-03,\n",
       "        2.7723959e-01, 2.2085395e-02],\n",
       "       [1.8037352e-04, 9.8982668e-01, 2.2430022e-04, 8.4401938e-05,\n",
       "        9.5383069e-03, 1.4591584e-04],\n",
       "       [2.7303877e-05, 6.1461215e-05, 4.7820024e-04, 7.3979894e-04,\n",
       "        4.0442785e-04, 9.9828881e-01],\n",
       "       ...,\n",
       "       [1.7109163e-04, 3.7865416e-04, 2.1471193e-03, 2.1336788e-02,\n",
       "        1.3310532e-02, 9.6265584e-01],\n",
       "       [4.4995941e-05, 1.9672023e-02, 3.9480557e-04, 1.5412706e-04,\n",
       "        9.7355169e-01, 6.1823567e-03],\n",
       "       [2.1220765e-05, 2.9467508e-05, 9.6603344e-06, 9.9880779e-01,\n",
       "        1.1285117e-03, 3.3434828e-06]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test_rgb)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "y_true = np.argmax(y_test_rgb, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test_rgb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(1,224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00259756, 0.68540215, 0.00805323, 0.00462149, 0.27724028,\n",
       "        0.02208538]], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40  1  0  0  0  0]\n",
      " [ 0 39  0  0  3  1]\n",
      " [ 0  0 49  1  0  0]\n",
      " [ 0  1  0 52  0  0]\n",
      " [ 0  1  0  0 48  0]\n",
      " [ 0  0  1  1  3 56]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99        41\n",
      "          1       0.93      0.91      0.92        43\n",
      "          2       0.98      0.98      0.98        50\n",
      "          3       0.96      0.98      0.97        53\n",
      "          4       0.89      0.98      0.93        49\n",
      "          5       0.98      0.92      0.95        61\n",
      "\n",
      "avg / total       0.96      0.96      0.96       297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_true, pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_true, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(model.predict(X_test_rgb), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_names = {0: 'C',\n",
    "                 1: 'Fist',\n",
    "                 2: 'L',\n",
    "                 3: 'Okay',\n",
    "                 4: 'Palm',\n",
    "                 5: 'Peace'}\n",
    "\n",
    "def predict_rgb_image_vgg(path):\n",
    "    img2rgb = image_utils.load_img(path=path, target_size=(224, 224))\n",
    "    img2rgb = image_utils.img_to_array(img2rgb)\n",
    "#     image_rgb.append(img2rgb)\n",
    "    img2rgb = img2rgb.reshape(1, 224, 224, 3)\n",
    "    return gesture_names[np.argmax(model.predict(img2rgb))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rgb_image_vgg('images_to_predict/test - palm.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/7 [================================] - 11s 1s/step - loss: 1.6103 - acc: 0.5610 - val_loss: 1.5952 - val_acc: 0.3856\n",
      "Epoch 2/10\n",
      "8/7 [================================] - 9s 1s/step - loss: 0.8910 - acc: 0.6878 - val_loss: 1.5901 - val_acc: 0.4364\n",
      "Epoch 3/10\n",
      "8/7 [================================] - 10s 1s/step - loss: 0.6128 - acc: 0.7967 - val_loss: 1.5757 - val_acc: 0.4492\n",
      "Epoch 4/10\n",
      "8/7 [================================] - 10s 1s/step - loss: 0.5136 - acc: 0.8229 - val_loss: 1.1850 - val_acc: 0.5508\n",
      "Epoch 5/10\n",
      "8/7 [================================] - 10s 1s/step - loss: 0.4274 - acc: 0.8570 - val_loss: 1.3173 - val_acc: 0.5424\n",
      "Epoch 6/10\n",
      "8/7 [================================] - 10s 1s/step - loss: 0.3949 - acc: 0.8673 - val_loss: 1.0941 - val_acc: 0.6017\n",
      "Epoch 7/10\n",
      "8/7 [================================] - 10s 1s/step - loss: 0.3201 - acc: 0.9048 - val_loss: 0.9776 - val_acc: 0.6483\n",
      "Epoch 8/10\n",
      "8/7 [================================] - 10s 1s/step - loss: 0.3264 - acc: 0.8904 - val_loss: 1.0325 - val_acc: 0.5847\n",
      "Epoch 9/10\n",
      "8/7 [================================] - 10s 1s/step - loss: 0.2715 - acc: 0.9132 - val_loss: 0.8119 - val_acc: 0.6568\n",
      "Epoch 10/10\n",
      "8/7 [================================] - 10s 1s/step - loss: 0.2702 - acc: 0.9018 - val_loss: 1.2239 - val_acc: 0.5847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ea56eaba8>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=45.,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(X_train_rgb)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(datagen.flow(X_train_rgb, y_train_rgb, batch_size=128),\n",
    "                    steps_per_epoch=len(X_train_rgb) / 128, epochs=10, validation_data=(X_test_rgb, y_test_rgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('drawing_VGG.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(X_test_rgb[0].reshape(1,224,224,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
