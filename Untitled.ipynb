{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from keras.applications import VGG16\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing import image as image_utils\n",
    "from keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [2, 4, 5, 6]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4],[2,4,5,6]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11b8c1390>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(a,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 4)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = a;\n",
    "G = a;\n",
    "B = a;\n",
    "b = np.array([R,G,B])\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11b70d128>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir frames/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_names = {0: 'thumb',\n",
    "                 1: 'palm_moved',\n",
    "                 2: 'l',\n",
    "                 3: 'palm',\n",
    "                 4: 'ok',\n",
    "                 5: 'down',\n",
    "                 6: 'index',\n",
    "                 7: 'c',\n",
    "                 8: 'fist',\n",
    "                 9: 'fist_moved'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_names = {0: 'Palm',\n",
    "                 1: 'L',\n",
    "                 2: 'Fist',\n",
    "                 3: 'Fist - Moved',\n",
    "                 4: 'Thumb',\n",
    "                 5: 'Index',\n",
    "                 6: 'OK',\n",
    "                 7: 'Palm - Moved',\n",
    "                 8: 'C',\n",
    "                 9: 'Down'}\n",
    "\n",
    "def predict_rgb_image(path):\n",
    "    img2rgb = image_utils.load_img(path=path, target_size=(224, 224))\n",
    "    img2rgb = image_utils.img_to_array(img2rgb)\n",
    "#     image_rgb.append(img2rgb)\n",
    "    img2rgb = img2rgb.reshape(1, 224, 224, 3)\n",
    "    result = gesture_names[model.predict_classes(img2rgb)[0]]\n",
    "    print(result)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Required argument 'filename' (pos 1) not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-fac115885612>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     cv2.imwrite(\"frames/frame%d.jpg\" % count, image)     # save frame as JPEG file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     success,image = vidcap.read()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mpredict_rgb_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Read a new frame: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Required argument 'filename' (pos 1) not found"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "vidcap = cv2.VideoCapture(0)\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "while success:\n",
    "#     cv2.imwrite(\"frames/frame%d.jpg\" % count, image)     # save frame as JPEG file      \n",
    "#     success,image = vidcap.read()\n",
    "    im = cv2.imread(mode='RGB')\n",
    "    predict_rgb_image(im)\n",
    "    print('Read a new frame: ', success)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-a6474e433441>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvidcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvidcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frames = 0\n",
    "while frames == 0:\n",
    "    vidcap = cv2.VideoCapture(0)\n",
    "    success,image = vidcap.read()\n",
    "    frame = imutils.resize(image, width=400)\n",
    "    frame = frame.transpose((2,0,1))\n",
    "    count = 0\n",
    "    image_count = 0\n",
    "\n",
    "# while image_count < 30:\n",
    "#     success,image = vidcap.read()\n",
    "#     if count % 30:\n",
    "#         cv2.imwrite(\"frames/frame%d.jpg\" % count, image)\n",
    "#         image_count += 1\n",
    "#     count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 270000 into shape (1,224,224,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-4fbb9a73e703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mframe2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 270000 into shape (1,224,224,3)"
     ]
    }
   ],
   "source": [
    "frame2 = frame.reshape(1, 224, 224, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2583\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2584\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2585\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-a81db735ddeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg2rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# img2rgb = image_utils.img_to_array(img2rgb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# #     image_rgb.append(img2rgb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# img2rgb = img2rgb.reshape(1, 224, 224, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# result = gesture_names[model.predict_classes(img2rgb)[0]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    496\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    497\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2584\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2586\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2587\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "img2rgb = image_utils.load_img(frame, target_size=(224, 224))\n",
    "# img2rgb = image_utils.img_to_array(img2rgb)\n",
    "# #     image_rgb.append(img2rgb)\n",
    "img2rgb = img2rgb.reshape(1, 224, 224, 3)\n",
    "result = gesture_names[model.predict_classes(img2rgb)[0]]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('my_model_with_augmentation.h5')\n",
    "# model = load_model('my_model_with_augmentation.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera OK\n",
      "[INFO] loading network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brenner/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1075, in _run\n",
      "    subfeed, allow_tensor=True, allow_operation=False)\n",
      "  File \"/Users/brenner/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3590, in as_graph_element\n",
      "    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\n",
      "  File \"/Users/brenner/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3669, in _as_graph_element_locked\n",
      "    raise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\n",
      "ValueError: Tensor Tensor(\"Placeholder:0\", shape=(3, 3, 3, 64), dtype=float32) is not an element of this graph.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/brenner/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-84-37f459b487c6>\", line 26, in run\n",
      "    self.model = VGG16(weights=\"imagenet\")\n",
      "  File \"/Users/brenner/anaconda3/lib/python3.6/site-packages/keras/applications/__init__.py\", line 28, in wrapper\n",
      "    return base_fun(*args, **kwargs)\n",
      "  File \"/Users/brenner/anaconda3/lib/python3.6/site-packages/keras/applications/vgg16.py\", line 11, in VGG16\n",
      "    return vgg16.VGG16(*args, **kwargs)\n",
      "  File \"/Users/brenner/anaconda3/lib/python3.6/site-packages/keras_applications/vgg16.py\", line 210, in VGG16\n",
      "    model.load_weights(weights_path)\n",
      "  File \"/Users/brenner/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\", line 1166, in load_weights\n",
      "    f, self.layers, reshape=reshape)\n",
      "  File \"/Users/brenner/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py\", line 1058, in load_weights_from_hdf5_group\n",
      "    K.batch_set_value(weight_value_tuples)\n",
      "  File \"/Users/brenner/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2470, in batch_set_value\n",
      "    get_session().run(assign_ops, feed_dict=feed_dict)\n",
      "  File \"/Users/brenner/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/Users/brenner/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1078, in _run\n",
      "    'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\n",
      "TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(3, 3, 3, 64), dtype=float32) is not an element of this graph.\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brenner/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "# from keras.preprocessing import image as image_utils\n",
    "# from imagenet_utils import decode_predictions\n",
    "# from imagenet_utils import preprocess_input\n",
    "# from vgg16 import VGG16\n",
    "# import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import threading\n",
    "\n",
    "label = ''\n",
    "frame = None\n",
    "\n",
    "class MyThread(threading.Thread):\n",
    "\tdef __init__(self):\n",
    "\t\tthreading.Thread.__init__(self)\n",
    "\n",
    "\tdef run(self):\n",
    "\t\tglobal label\n",
    "\t\t# Load the VGG16 network\n",
    "\t\tprint(\"[INFO] loading network...\")\n",
    "\t\tself.model = VGG16(weights=\"imagenet\")\n",
    "\n",
    "\t\twhile (~(frame is None)):\n",
    "\t\t\t(inID, label) = self.predict(frame)\n",
    "\n",
    "\tdef predict(self, frame):\n",
    "\t\timage = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "\t\timage = image.transpose((2, 0, 1))\n",
    "\t\timage = image.reshape((1,) + image.shape)\n",
    "\n",
    "\t\timage = preprocess_input(image)\n",
    "\t\tpreds = self.model.predict(image)\n",
    "\t\treturn decode_predictions(preds)[0]\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if (cap.isOpened()):\n",
    "\tprint(\"Camera OK\")\n",
    "else:\n",
    "\tcap.open()\n",
    "\n",
    "keras_thread = MyThread()\n",
    "keras_thread.start()\n",
    "\n",
    "while (True):\n",
    "\tret, original = cap.read()\n",
    "\n",
    "\tframe = cv2.resize(original, (224, 224))\n",
    "\n",
    "\t# Display the predictions\n",
    "\t# print(\"ImageNet ID: {}, Label: {}\".format(inID, label))\n",
    "\tcv2.putText(original, \"Label: {}\".format(label), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\tcv2.imshow(\"Classification\", original)\n",
    "\n",
    "\tif (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "\t\tbreak;\n",
    "\n",
    "cap.release()\n",
    "frame = None\n",
    "cv2.destroyAllWindows()\n",
    "sys.exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
